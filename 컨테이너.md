# 컨테이너란?
- 애플리케이션과 실행환경(라이브러리, 설정 등)을 패키징해서 격리 된 공간에서 실행되는 리눅스 프로세스
	- **리눅스 커널의 [[Namespace, cgroups]], croups, overlayFS 기능을 활용한 격리 기술 (커널 기능 조합)**
- 가상머신은 OS 전체를 가상화, 컨테이너는 호스트 커널을 공유하면서 그 밖의 실행 환경 등은 격리되는 방식
	- OS를 포함하지 않아 가볍고 빠름
	- 실행환경이란?
		- 실행 바이너리(nginx), 라이브러리, 설정 파일(nginx.conf), 사용자 디렉토리, 환경변수 등 앱이 실행되기 위해 필요한 모든 것
- 사용 목적
	- 실행환경 일치 (이미지에 실행 환경이 다 패키징 되어 있음)
	- 커널 공유로 효율적인 자원 활용
### 도커, 쿠버네티스, 컨테이너 차이
- 컨테이너는 리눅스 커널의 namespace, cgroups, 파일시스템을 활용한 격리 기술
- 도커는 위의 기능을 조합해서 다룰 수 있도록 하는 오픈소스 도구
	- 오버레이 마운트, cgroups  설정 자동화 등을 명령어로 쉽게 설정하도록 함 
- 도커/쿠버네티스 차이
	- 도커: 도커 엔진이 커널 기능 직접 제어 (도커가 직접 네임스페이스, cgroups 등 설정)
	- 쿠버네티스: 컨테이너 런타임이 커널 기능을 사용 (사용자가 컨테이너 런타임에 요청) 
# 컨테이너 (layer 구조)
## 컨테이너 이미지
- 레이어 방식으로 구성
	- overlayFS가 이 레이어를 하나의 파일시스템으로 합쳐져 보이게 함
		- 도커는 각각의 레이어를 `/var/lib/docker/overlay2` 아래에 개별적으로 저장
- 레이어 구성
	- 읽기 전용 이미지 레이어 (lowerdir)
	- 쓰기 레이어 (upperdir)
	- 사용자에게 보이는 최종 파일시스템 (merged)
- 구성 과정
	- 기존 이미지 레이어는 lowerdir로 설정, 컨테이너 실행 시 upperdir(쓰기 영역) 생성 됨
	- overlayFS로 lower+upper 를 합쳐서 merged 생성
	- merged 디렉토리를 컨테이너의 루트로 마운트
		- 해당 구조를 `/var/lib/docker/overlay2`에 저장
- 레이어 방식 구성 장점
	- 디스크 공간 절약
		- 같은 base layer를 사용하는 경우 중복으로 저장하지 않고 컨테이너끼리 서로 공유
	- 변경 사항만 저장 (CoW)
		- 이미지 자체 수정X, 변경 사항은 upper layer에 기록
#### 도커 이미지 레이어 구조
- 레이어 구조 예시 (ex. nginx)
```
Layer 1: debian base
Layer 2: apt update
Layer 3: nginx 설치
Layer 4: 설정파일 추가
```
- 이미지 저장 방식
	- 베이스 이미지는 `/var/lib/docker/overlay2/`에 한 번만 저장
	- 각 컨테이너는 따로 쓰기 가능 영역 (diff/)가 생김
	- 즉 읽기 레이어는 공통으로 재사용하고, 쓰기 레이어만  컨테이너 별로 따로 생성
```
/var/lib/docker/overlay2/
├── abc123/   ← Layer 1: Ubuntu base
├── def456/   ← Layer 2: nginx
├── ghi789/   ← Layer 3: app 설정

컨테이너 A: upperdir = /overlay2/aaa111/diff
컨테이너 B: upperdir = /overlay2/bbb222/diff
```
- OverlayFS 구성 방식
	- 여러 개의 레이어(읽기전용)을 : 로 이어 붙여서 **하나의 논리적 lowerdir**로 구성
		- 도커가 이미지 레이어를 lowerdir로 참조할 수 있도록 만들어 놓은 링크용 디렉터리 / 실제 내용x
	- 따라서 각 레이어는 서로 분리 된 상태로 저장
		- 이미지는 컨테이너 단위로 저장되지 않고 이미지 단위로 한 번만 저장
```
lowerdir=/var/lib/docker/overlay2/l/<layer4>:<layer3>:<layer2>:<layer1>
```
- 이미지 레이어 구조
	- 컨테이너 id/: 컨테이너 별로 생성 되는 디렉토리
	- image-layer-id/: lower 이미지 레이어
	- l/ : 각각의 이미지에서 lower 레이어가 어떻게 연결되어 있는지
```
# 이미지 레이어 (읽기 전용)
overlay2/
├── l/
│   └── sha256:<hash>   ← lowerdir 레이어 참조용 링크 디렉터리
│        ↘ 가리키는 대상
├── <image-layer-id>/ (ex.ubuntu base)
│   ├── diff/           ← 해당 레이어의 실제 내용
│   └── link            ← 위 l/ 디렉터리에서 참조됨

# 컨테이너 실행 시 생성되는 쓰기 영역 (upper)
├── <container-layer-id>/
│   ├── diff/           ← upperdir (쓰기 가능한 공간)
│   ├── merged/         ← OverlayFS 결과 (컨테이너의 /)
│   └── work/           ← overlayfs 내부 처리 공간
```
#### 컨테이너 저장 구조
- 컨테이너는 하드웨어 가상화 방식이 아니기 때문에 "디렉토리 트리"가 가상디스크 역할을 함
	- overlayFS가 레이어를 합쳐 하나의 루트 디렉토리로 마운트 -> 실제 컨테이너의 디스크 역할
	- `/var/lib/docker/overlay2/`에 저장
## 동작 원리
- 컨테이너는 아래의 리눅스 커널 기능을 활용한 기술
	1) Namespace (격리)
		- 컨테이너가 각각 자신만의 프로세스, 네트워크, 파일시스템 등을 갖는 것처럼 보이게 함
	2) Cgroups (자원 제한)
		- 사용할 CPU, 메모리, 디스크I/O 등 하드웨어 자원 제한
	3) Union Filesystem / OverlayFS (파일시스템)
		- 여러 디렉토리를 하나의 계층적 구조로 합쳐셔 보여주는 파일 시스템
			- UnionFilesystem: 여러 디렉토리를 union mount하는 파일 시스템
			- OverlayFS: unionFS의 한 종류, 도커/컨테이너에서 실제 사용
				- 리눅스 커널에 포함
	4) Capabilities
		- 루트 권한을 쪼개서 필요한 권한만 허용하는 것
	5) seccomp
		- 시스템 콜 필터링
	6) SELinux
## 디렉터리 구조
### 이미지-컨테이너 디스크 구조
- image /
	- 이미지와 컨테이너 실행 정보 추적/연결하기 위한 메타데이터 저장 경로
- containers/
	- 도커 컨테이너 메타데이터 정보 저장
	- 컨테이너의 구성, 상태, 실행 방식 등 핵심 정보
- overlay2/`<이미지-layer-id>`/diff
	- lower 레이어의 파일 내용이 실제 저장된 곳
		- ex) `/usr/sbin/nginx`
- 도커 볼륨
	- 컨테이너의 upper 계층은 컨테이너가 삭제되면 같이 삭제 됨
		- 컨테이너 외부에 데이터가 저장되는 영역
			- ex. mysql 데이터는 보통 `/var/lib/mysql`에 저장
			- 외부 볼륨을 마운트 하여 사용하면 컨테이너가 사라져도 데이터 유지 가능 / 새 컨테이너가 해당 볼륨 마운트하면 동일하게 사용 가능
```
/var/lib/docker/
├── overlay2/                    ← 이미지 및 컨테이너 레이어 저장소 (중심)
│   ├── <이미지-layer-id>/      ← 읽기 전용 이미지 레이어
│   │   ├── diff/                ← 해당 레이어의 실제 내용
│   │   └── link                 ← l/ 디렉토리에서 참조용 symlink
│   │
│   ├── l/                       ← 이미지 레이어 참조용 링크 디렉토리
│   │   └── sha256:<hash> → ../<이미지-layer-id>/link 로 연결
│   │
│   ├── <컨테이너-layer-id>/    ← 컨테이너 실행 시 만들어지는 upper layer
│   │   ├── diff/                ← upperdir (쓰기 가능)
│   │   ├── merged/              ← OverlayFS 결과 (컨테이너 내에서 보이는 /)
│   │   └── work/                ← OverlayFS 작업 디렉터리 (필수)
│
├── containers/
│   └── <컨테이너-id>/          ← 컨테이너 메타데이터 저장
│       ├── config.v2.json       ← 실행 커맨드, 네트워크, 환경변수 등
│       ├── hostconfig.json      ← 호스트 측 볼륨, 제한 등 설정
│       └── log.json             ← stdout/stderr 로그

├── image/
│   └── overlay2/
│       └── layerdb/
│           ├── sha256/          ← 이미지 레이어 메타데이터
│           └── mounts/
│               └── <컨테이너-id>/mount-id → overlay2/<container-layer-id>
│
├── volumes/                     ← 도커 볼륨 저장 위치
├── network/                     ← 브리지, NAT, 커스텀 네트워크 설정
└── buildkit/                    ← 도커 이미지 빌드 관련 정보
```
### 레이어 구조 요약
- 이미지 레이어
	- 읽기 전용
	- 여러 레이어가 stack으로 쌓여 있음 (컨테이너가 서로 공유)
	- 프로그램 코드, 바이너리, 설정 등
- 컨테이너 레이어
	- 쓰기 가능, 일시적
	- 컨테이너에서 변경한 임시 파일
	- 이미지 레이어 위에 오버레이로 쌓임
- 볼륨
	- 영구 데이터
## 도커 볼륨
- 컨테이너가 삭제되면 데이터도 함께 삭제 되기 때문에 주로 외부 볼륨에 마운트 해서 저장
- 종류
	- 바인드 마운트
		- 호스트의 디렉터리에 마운트, 경로 직접 지정
	- 도커 볼륨 (주로 사용)
		- 도커가 자동 관리 (`/var/lib/docker/volumes/<볼륨이름>/_data`)
		- 권한, 컨테이너와의 충돌 문제 등으로 인해 외부에서 직접 접근은 권장X
- 명령어 예시
	- `docker volume create myvolume`
	- 실제 저장 경로
		- `/var/lib/docker/volumes/myvolume/_data/` (호스트 저장 위치)
		- 컨테이너에서는 docker run 할 때 볼륨을 마운트 한 위치로 보임
	- 만일 docker run 할 때 볼륨명을 지정하지 않으면 랜덤 생성
- 컨테이너 간 볼륨 공유
	- 동일한 볼륨 이름 사용
## 실행 흐름
1. docker run nginx (nginx 이미지 러닝)
2. 도커가 명령한 nginx 이미지가 있는지 확인
	- 없다면 docker hub에서 다운로드
3. 읽기 전용 이미지 레이어 저장 (lower)
	- `/var/lib/docker/overlay2/<imgae-layer-id>/diff/`
	- `/var/lib/docker/overlay2/l/`
		- 레이어 참조용 링크
4. 컨테이너 전용 upper layer 디렉토리 생성
	- `/var/lib/docker/overlay2/<container-layer-id>/diff/`
	- `/var/lib/docker/overlay2/<container-layer-id>/work/`
		- overlayfs 작업 디렉터리
 5. overlayFS로 upper layer와 lower layer를 합쳐서 하나의 루트 파일시스템으로 만듦
	 - `/var/lib/docker/overlay2/<container-layer-id>/merged/`
 6. 네임스페이스로 자원 격리
 7. cgroups로 자원 제한
	 - cgroups 설정을 통해 CPU, 메모리, 블록I/O 제한
	 - 실제 설정은 `/sys/fs/cgroup/`저장
 8. 네트워크 인터페이스 연결 (veth 쌍 생성)
	 - 각각 컨테이너 네트워크 네임스페이스로
	 - docker0 브리지로 연결
	 - NAT/IP 할당
			 - dnsmasq + iptables를 통해 자동 처리
 9. 컨테이너 프로세스 실행
	 - `/var/lib/docker/overlay2/<container-layer-id>/merged` 경로를 컨테이너의 루트로 마운트

# 컨테이너 구성 (데몬)
- 컨테이너 주요 데몬
	- dockerd
	- containered
	- runc
- dockerd
	- 도커의 메인 데몬 (kvm의 libvirt와 같은 역할)
	- 도커 CLI가 명령을 보내는 중앙 컨트롤러 역할 
		- rest API 요청을 받아서 처리 (원격 서버에서도 동일하게 컨테이너 관리 가능)
			- rest API: http 기반 API
	- 이미지 및 컨테이너 생성/삭제/네트워크 설정 / 볼륨 관리
- containerd
	- dockerd 안에 포함 (독립 실행 가능)
		- dockered가 containerd에게 컨테이너 생성을 요청하는 구조
	- 컨테이너 실행, 중지, 스냅샷 관리 등 저수준 컨테이너 관리 담당
- runc
	- containerd가 사용하는 OCI 표준 런타임
	- 네임스페이스, cgroups, 루트파일시스템을 최종 세팅하여 **실제로 컨테이너 프로세스를 실행**
```
[ docker CLI ]
       ↓
[ dockerd ] --> 전반적인 관리
       ↓
[ containerd ] ----> 컨테이너 생명 주기
       ↓
[ runc ] ----> 실제 프로세스 실행
       ↓
리눅스 커널 (네임스페이스, cgroups 등)
```
## 컨테이너 구성 흐름
1. 사용자가 도커 명령어 입력
2. docker cli가 dockered의 rest api로 요청 전송
3. dockerd가 아래 정보 확인
	- 컨테이너 메타데이터, 네트워크, 볼륨 이미지 등
	- 네트워크 드라이버에게 네트워크 구성 명령
	- containered 데몬에게 컨테이너 생성 요청
4. containered
	- 컨테이너 관리 담당 (생성, 실행, 중지 등)
	- OCI 스펙 JSON을 runc에게 전달
		- 컨테이너 실행에 필요한 스펙을 담은 json 파일
		- 런타임이 json파일을 읽고 네임스페이스 분리, cgroups 설정, 루트 파일시스템 마운트 등을 수행
5. runc
	- 네임스페이스, cgroups, 루트 파일시스템 마운트 등
	- 실제 컨테이너 프로세스 fork/exec
		- fork: 격리 된 네임스페이스, cgroups안에 새로운 프로세스 생성
		- exec: 프로세스에서 컨테이너의 메인 프로세스 실행
6. 리눅스 커널
	- 컨테이너를 독립된 프로세스처럼 실행
# 네트워크 (도커)
- 컨테이너는 네트워크 네임스페이스를 사용해서 호스트와 분리 된 자신만의 네트워크 스택 (IP,라우팅,포트 등)을 갖도록 설계 됨
	- 컨테이너 안에서는 자신의 인터페이스만 보임
	- 호스트 네임 스페이스 / 컨테이너 네임 스페이스
- 도커가 네트워크 드라이버를 사용하여 관리
	- 네트워크 드라이버란?
		- 도커의 네트워크 구성 담당 모듈
		- 플러그인으로 추가 가능(CNI)
		- `도커 명령어 -> dockered -> 네트워크 드라이버에게 네트워크 구성 명령`
	- 아무 옵션 없을 경우 브리지로 설정
	- 다른 모드 사용 시 `--network`옵션 지정하여 컨테이너 생성
## 네트워크 드라이버
- Bridge (default)
	- `docker0`라는 가상 브리지에 컨테이너가 연결 (L2역할)
		- 컨테이너에 사설 ip 부여
			- 같은 브리지 네트워크에 있으면 컨테이너끼리 사설 IP로 통신
			- 서로 다른 가상 브리지에 연결 시 라우팅 필요
		- `docker0`은 호스트 네임스페이스에 존재하는 가상 네트워크 인터페이스
			- 커스텀 브리지 생성 시 실제 호스트의 네트워크 네임스페이스에 `br-xxxx`가 생성 (ip link, brctl show로 확인)
	- 외부에서 컨테이너 접속 시 포트포워딩 사용
		- 호스트의 특정 포트로 들어오는 트래픽을 컨테이너의 특정 포트로 연결
- host
	- 컨테이너가 호스트의 네트워크 네임스페이스를 그대로 사용
		- 생성 시 `--network host` 옵션 부여
		- 여러 컨테이너 동시에 사용 불가
	- 컨테이너 포트 = 호스트 포트
		- 호스트의 NIC을 그대로 사용 / 컨테이너에서 호스트에서 열려 있는 포트 조회 가능
	- 오버헤드가 적으며 성능이 중요할 때 주로 사용 
- none
	- 네트워크 인터페이스 없음 / 격리 구조
- overlay
	- 여러 호스트 간의 컨테이너 네트워크를 가상으로 연결
		- 여러 물리 머신 간 컨테이너 연결
	- VXLAN 기반
	- 쿠버네티스 같은 오케스트레이션 환경에서 주로 사용
- macvlan
	- 컨테이너에 실제 물리 NIC과 같은 고유한 MAC주소 부여
	- 물리 네트워크 상에서 하나의 독립 된 장치처럼 동작하도록 함
## Bridge
- 브리지 구조
	- 사설 IP대역은 생성 시 지정하여 변경 가능
	- 생성 후에는 변경 불가
- 호스트와 NAT로 연결
- 브리지는 호스트 네임스페이스 안에 존재하는 가상 인터페이스
	- 호스트 NIC과는 직접적으로 연결X
```
[Host NIC] ---- [docker0 bridge] ---- [veth pair] ---- [container eth0]
```
- 연결 방식
```
  [컨테이너 eth0]
        ↓
      veth pair
        ↓
  [docker0 브리지]  (가상 스위치)
        ↓
  [호스트 라우팅 + iptables NAT]
        ↓
  [호스트 물리 NIC (eth0)]
        ↓
     외부 네트워크
```
- 커스텀 브리지 생성 시
```
                                 [ 호스트 네트워크 네임스페이스 ]
 ┌─────────────────────────────────────────────────────────────────────────────┐
 │                                                                             │
 │  eth0 (192.168.1.100)      ← 실제 호스트 물리 NIC                          │
 │                                                                             │
 │  docker0 (172.17.0.1)  ← 기본 브리지                                        │
 │       │                                                                  │
 │       │── vethA0 ───────────────────────────┐                            │
 │       │                                     │                            │
 │       │── vethB0 ───────────────────────────┘                            │
 │                                                                             │
 │  br-xxxxxx (192.168.100.1)  ← 커스텀 브리지 mybridge                        │
 │       │                                                                  │
 │       │── vethC0 ───────────────────────────┐                            │
 │       │                                     │                            │
 │       │── vethD0 ───────────────────────────┘                            │
 │                                                                             │
 └─────────────────────────────────────────────────────────────────────────────┘
          ↓
 ┌───────────────────────┐                ┌───────────────────────┐
 │ 컨테이너 A (eth0: 172.17.0.2)│          │ 컨테이너 C (eth0: 192.168.100.2)│
 └───────────────────────┘                └───────────────────────┘
 ┌───────────────────────┐                ┌───────────────────────┐
 │ 컨테이너 B (eth0: 172.17.0.3)│          │ 컨테이너 D (eth0: 192.168.100.3)│
 └───────────────────────┘                └───────────────────────┘
```
- veth(Virtual Ethernet Pair)
	- 항상 쌍으로 생성 됨
	- 한 쪽은 호스트 네임스페이스 / 한 쪽은 컨테이너 네임스페이스에 연결
- iptables 설정
	- 기본 브리지/커스텀 브리지 모두 dockered 데몬이 NAT 자동 구성
